{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdsmI8ZmWb1_",
        "outputId": "ab9b9700-792c-4b59-92de-ff7cd07b9aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krd8Tb2w6-bI",
        "outputId": "f22cf161-1f8c-49e2-f0b1-ebfb4c889dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ijjfQCXD7ELH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D-iPpgA57Rme"
      },
      "outputs": [],
      "source": [
        "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VcEtNG_07ad2"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(dataset.data), columns=['text'])\n",
        "df[\"label\"] = list(dataset.target)\n",
        "label_mapping = {}\n",
        "for i in range(20):\n",
        "  label_mapping[i] = dataset.target_names[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkVSLT_1m8oG"
      },
      "source": [
        "Extractive Summarization using spacy and assigning weights based on word frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aq1JS9oS7hzd"
      },
      "outputs": [],
      "source": [
        "def summarize1(text, per):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc= nlp(text)\n",
        "    tokens=[token.text for token in doc]\n",
        "    word_frequencies={}\n",
        "    for word in doc:\n",
        "        if word.text.lower() not in list(STOP_WORDS):\n",
        "            if word.text.lower() not in punctuation:\n",
        "                if word.text not in word_frequencies.keys():\n",
        "                    word_frequencies[word.text] = 1\n",
        "                else:\n",
        "                    word_frequencies[word.text] += 1\n",
        "    max_frequency=max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
        "    sentence_tokens= [sent for sent in doc.sents]\n",
        "    sentence_scores = {}\n",
        "    for sent in sentence_tokens:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequencies.keys():\n",
        "                if sent not in sentence_scores.keys():                            \n",
        "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
        "    select_length=int(len(sentence_tokens)*per)\n",
        "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
        "    final_summary=[word.text for word in summary]\n",
        "    summary=''.join(final_summary)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lsypJAalsxJ"
      },
      "source": [
        "Abstractive Summarization using Facebook Bart Large CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0PlL-SAhWLLw"
      },
      "outputs": [],
      "source": [
        "def summarize2(text, per):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc= nlp(text)\n",
        "  sentence_tokens= [sent for sent in doc.sents]\n",
        "  select_length = int(len(sentence_tokens)*per)\n",
        "  summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summarizer(text, max_length=130, min_length=30)\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cluster summarization\n",
        "clusters = df['label'].values\n",
        "final_text = \"\"\n",
        "final_summary = \"\"\n",
        "for cluster in clusters:\n",
        "  prev_doc = \"\"\n",
        "  filtered = df.loc[df['label'] == cluster]\n",
        "  for doc in filtered['text']:\n",
        "    final_text += doc\n",
        "    if len(prev_doc) + len(doc) < 2700:\n",
        "      prev_doc += doc\n",
        "      prev_doc += \". \"\n",
        "      continue\n",
        "    # summary = summarize1(text[:1000000], 0.01)\n",
        "    try:\n",
        "      summary = summarize2(prev_doc, 0.01)\n",
        "    except:\n",
        "      summary = summarize2(prev_doc[:1500], 0.01)\n",
        "    final_summary += summary[0]['summary_text']\n",
        "    prev_doc = doc\n",
        "  print(final_summary)\n",
        "  filename = label_mapping[cluster]\n",
        "  filename = filename.replace(\".\", \"_\")\n",
        "  filename += \"_summary.txt\"\n",
        "  filepath = \"./drive/MyDrive/\" + filename\n",
        "  f = open(filepath, \"w\")\n",
        "  f.write(final_summary)\n",
        "  f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
